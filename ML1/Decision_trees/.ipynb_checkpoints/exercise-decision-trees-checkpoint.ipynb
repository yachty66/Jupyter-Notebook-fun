{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# ML-Fundamentals - Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Table of Contents\n",
    "* [Introduction](#Introduction)\n",
    "* [Requirements](#Requirements) \n",
    "  * [Knowledge](#Knowledge) \n",
    "  * [Modules](#Python-Modules)\n",
    "* [Teaching Content](#Teaching-Content)\n",
    " * [Decision Trees](#Decision-Trees)\n",
    " * [Learning](#Learning)\n",
    " * [Information Gain as Splitting Criterion](#Information-Gain-as-Splitting-Criterion)\n",
    " * [Overfitting](#Overfitting)\n",
    "* [Exercises](#Exercises)\n",
    " * [Loading the Data](#Loading-the-Data)\n",
    " * [Preprocessing](#Preprocessing)\n",
    " * [Training and Visualization](#Training-and-Visualization)\n",
    " * [Prediction / Validation](#Prediction-/-Validation)\n",
    " * [Crossvalidation](#Crossvalidation)\n",
    " * [Gridsearch](#Gridsearch)\n",
    " * [Splitting Criterion Entropy / Information Gain](#Splitting-Criterion-Entropy-/-Information-Gain)\n",
    "* [Summary and Outlook](#Summary-and-Outlook)\n",
    "* [Literature](#Literature) \n",
    "* [Licenses](#Licenses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this exercise you will train and evaluate a decision tree. Opposed to the previous exercises with the topics _univariate linear regression_, _multivariate linear regression_, _logistic regression_ and _bias variance tradeoff_, you will not implement the algorithms from scratch using numpy. Instead you will use two python packages written on top of numpy, namely the _pandas_ and the _scikit-learn_ package, which are both widely used by the machine learning community. The steps can be broken down into:\n",
    "\n",
    "1. Load the data from a _csv file_ into a pandas DataFrame object\n",
    "2. Preprocess the data\n",
    "3. Train a decision tree and visualize it.\n",
    "4. Do hyperparameter optimization by dividing the dataset into a fixed training set and a fixed validation set.\n",
    "5. Do hyperparameter optimization by crossvalidation\n",
    "6. Do hyperparameter optimization by gridsearch\n",
    "\n",
    "Afterwards, show that you understand, what computations are done by the _scikit-learn_ package you have used by manually computing:\n",
    "\n",
    "7. The entropy for one node\n",
    "8. The information gain for one node\n",
    "\n",
    "**Note:**\n",
    "\n",
    "As this exercise heavily focuses on the usage of the high-level APIs _pandas_ and _scikit-learn_, reading their documentations (just the parts corresponding to the current task) is strongly recommended:\n",
    "\n",
    "- https://scikit-learn.org/stable/documentation.html#\n",
    "- https://pandas.pydata.org/pandas-docs/stable/search.html?q=&check_keywords=yes&area=default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Requirements\n",
    "### Knowledge\n",
    "\n",
    "You should have a basic knowledge of:\n",
    "- Decision Trees\n",
    "- Entropy\n",
    "- Information gain\n",
    "- Crossvalidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Python Modules\n",
    "\n",
    "By [deep.TEACHING](https://www.deep-teaching.org/) convention, all python modules needed to run the notebook are loaded centrally at the beginning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#from sklearn.preprocessing import Imputer # in newer versions: \n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "#old: from sklearn.externals.six import StringIO\n",
    "from six import StringIO\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from graphviz import Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teaching Content\n",
    "\n",
    "### Decision Trees for Classification\n",
    "\n",
    "In a decision tree the data is split at each node according to a decision rule. This corresponds to nested *if-then-else*-rules. In the *if*-part of such a rule are decision is made based on a feature of the data record. \n",
    "\n",
    "We will use the scikit learn implementation. For this implementation the features must be binary or have (at least) ordinal characteristic. If a feature is e.g. nominal with many values, it must be converted to a set of binary (one-hot-coded) features.\n",
    "\n",
    "\n",
    "The splitting rules in the scikit learn implementation are binary and are based on a threshold, e.g.\n",
    "  - _if $x_6 <= 2.14$ then_ left subbranch, *else* right subbranch.         \n",
    "  - binary features must be coded as 0/1, so the rule becomes: if $x_2 <= 0.5$ _then_ left subbranch, _else_ right subbranch. \n",
    "\n",
    "\n",
    "<!--The structure of the tree will be determined by data (see below) and a training procedure.-->\n",
    "\n",
    "In the leaves of the tree the (class) predictions are made. There are two possibilities for such an inference: \n",
    "   - hard assignment: Predict for the data records which end up on a leaf by the majority class of the training data that end up on that leaf.          \n",
    "   - soft assignment: Assign probabilities according to the distribution of the classes in respect to the training data which end up on that leaf.\n",
    "\n",
    "As an example of a decision tree we will learn the following tree from the titanic data set: \n",
    "\n",
    "<img src=\"https://gitlab.com/deep.TEACHING/educational-materials/raw/master/media/klaus/exercise-decision-trees-a-tree.png\" width=\"1024\" alt=\"internet connection needed\">\n",
    "\n",
    "A full explanation of the tree will be given later. Here just look at the decision rules (first line of the inner nodes) and at the last line of the leafs. In each leaf you see an array (values) with counts of the different targets for the train data: [number_died, number_survivors] .\n",
    "\n",
    "### Learning \n",
    "\n",
    "Finding a tree that splits the training data optimal is [np-hard](https://en.wikipedia.org/wiki/NP-hardness). Therefore often a *greedy*-strategy is used:\n",
    "\n",
    "To build up a decision tree the algorithm starts at the root of the tree. The feature and the threshold \n",
    "that splits the training data best (with respect to the classes) are chosen. In an iterative way the whole tree is build up by such splitting rules. \n",
    "\n",
    "There are different criteria for measuring the \"separation (split) quality\". The most important ones are:\n",
    "\n",
    "- Gini Impurity \n",
    "- Information Gain \n",
    "\n",
    "In this tutorial we concentrate on the information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain as Splitting Criterion\n",
    "\n",
    "The entropy with respect to the target class variable $y$ of a training data set $\\mathcal D$ is defined as:\n",
    "\n",
    "$$\n",
    " H(y, \\mathcal D) = - \\sum_{y \\in \\mathcal Y} p(y|\\mathcal D) \\log_2 p(y|\\mathcal D)\n",
    "$$\n",
    "with the domain of the target values $\\mathcal Y = \\{t_1, t_2,... \\}$.\n",
    "\n",
    "\n",
    "The probabilities are estimated by \n",
    "$$\n",
    "  p(y=t_i, \\mathcal D) = |\\mathcal D^{(y=t_i)}| /|\\mathcal D| \n",
    "$$    \n",
    "\n",
    "\n",
    "with the number of training data $|\\mathcal D|$  and the number of training data $|\\mathcal D^{(y=t_i)}|$ with target label $t_i$: \n",
    "\n",
    "\n",
    "On a node a (binary) split on a feature $x_k$ is made by the split rule $x_k \\leq v$. \n",
    "As result there are two data sets $\\mathcal D_0$ and $\\mathcal D_1$ for the left resp. the right branch.\n",
    "\n",
    "The feature $x_k$ and the split value $v$ are chosen that they maximize the 'reduction of the entropy' measured by the information gain $I$:\n",
    "$$\n",
    "  I(y; x_k) = H(y, \\mathcal D) - H(y|x_k) = H(y, \\mathcal D) - \\sum_{j=0}^1 p_jH(y, \\mathcal D_j) =\n",
    "  H(y, \\mathcal D) + \\sum_{j=0}^1  \\sum_{y \\in \\mathcal Y} \\frac{|\\mathcal D_j|}{|\\mathcal D|} p(y|\\mathcal D_j) \\log_2 p(y|\\mathcal D_j)\n",
    "$$\n",
    "Note that $p_{j=0}$  is the estimated probability that a random data record of $\\mathcal D$ has feature value $x_k \\leq v$ which can be estimated by ${|\\mathcal D_0|}/{|\\mathcal D|}$ (analog for $j=1$).\n",
    "\n",
    "$p(y=t_i|\\mathcal D_0)$ can also be estimated by the fraction of the counts ${|\\mathcal D_0^{(y=t_i)}|}/{|\\mathcal D_0|}$. \n",
    "So the information gain can be computed just with counts:\n",
    "\n",
    "\n",
    "$$\n",
    "  I(y; x_k) = -\n",
    "   \\sum_{y \\in \\mathcal Y} \\frac{|\\mathcal D^{(y=t_i)}|}{|\\mathcal D|}  \\log_2 \\frac{|\\mathcal D^{(y=t_i)}|}{|\\mathcal D|} + \\sum_{j=0}^1  \\sum_{y \\in \\mathcal Y} \\frac{|\\mathcal D_j^{(y=t_i)}|}{|\\mathcal D|}  \\log_2 \\frac{|\\mathcal D_j^{(y=t_i)}|}{|\\mathcal D_j|}\n",
    "$$\n",
    "\n",
    "\n",
    "<!--$|\\mathcal D_0|$ respectivly $|\\mathcal D_1|$ is the number of elements in the splitted data sets.-->\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "Deep decision trees generalize often poorly. The following remedies reduce overfitting: \n",
    "\n",
    "- Limitation of the maximal depth of the tree. \n",
    "- Pruning with an validation set either during training (pre-pruning) or after training (post-pruning).\n",
    "- Dimensionality reduction (reducing the number of features before training)\n",
    "\n",
    "\n",
    "Also often combining decision trees to an ensemble (decision forests) is used against overfitting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "\n",
    "First we read in the _titanic data set_ with _pandas_\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. Complete the code to load the dataset as [_pandas_](http://pandas.pydata.org/) [DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html) object.\n",
    "2. Either download the [_titanic-train.csv_](https://gitlab.com/deep.TEACHING/educational-materials/raw/master/datasets/titanic-train.csv) or alternatively read the url directly into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Load the csv file as pandas DataFrame\n",
    "url = 'https://gitlab.com/deep.TEACHING/educational-materials/raw/master/notebooks/data/titanic-train.csv'\n",
    "train_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataFrame` class implements (and overwrites) a lot of (standard) methods. Some of the are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(891, 12)\n"
     ]
    }
   ],
   "source": [
    "### Not an exercise, just execute the cell\n",
    "\n",
    "print(train_df.ndim)\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Not an exercise, just execute the cell\n",
    "\n",
    "### To view the dataset as pretty table (when using jupyter) \n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 3, ..., 7.25, nan, 'S'],\n",
       "       [2, 1, 1, ..., 71.2833, 'C85', 'C'],\n",
       "       [3, 1, 3, ..., 7.925, nan, 'S'],\n",
       "       ...,\n",
       "       [889, 0, 3, ..., 23.45, nan, 'S'],\n",
       "       [890, 1, 1, ..., 30.0, 'C148', 'C'],\n",
       "       [891, 0, 3, ..., 7.75, nan, 'Q']], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Not an exercise, just execute the cell\n",
    "\n",
    "### To return the data as numpy array:\n",
    "train_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 'male'],\n",
       "       [2, 1, 'female'],\n",
       "       [3, 1, 'female'],\n",
       "       ...,\n",
       "       [889, 0, 'female'],\n",
       "       [890, 1, 'male'],\n",
       "       [891, 0, 'male']], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Not an exercise, just execute the cell\n",
    "\n",
    "### To return just certain features (columns)\n",
    "train_df[['PassengerId', 'Survived', 'Sex']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "#### Feature Transformation\n",
    "\n",
    "Scikit's learn decision trees can handle only numeric data. So we must convert the nominal `Sex` feature. \n",
    "\n",
    "**Task:**\n",
    "\n",
    "Convert the nominal feature into a binary feature. Convert 'male' to 0 and 'female' to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        male\n",
       "1      female\n",
       "2      female\n",
       "3      female\n",
       "4        male\n",
       "        ...  \n",
       "886      male\n",
       "887    female\n",
       "888    female\n",
       "889      male\n",
       "890      male\n",
       "Name: Sex, Length: 891, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Exercise: Convert to binary feature\n",
    "train_df[\"Sex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"Sex\"] = train_df[\"Sex\"].apply(lambda sex: 1 if sex == 'female' else 0)\n",
    "# or\n",
    "#df.Sex[df[\"Sex\"]=='female'] = 1\n",
    "#df.Sex[df[\"Sex\"]=='male'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert train_df[\"Sex\"].values.sum() == 314"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "\n",
    "`Survived` is the target, that we want to predict from the values of the other columns.   \n",
    "But not all of the other columns are helpful for classification. So we choose a feature set by hand and convert the features into a numpy array for scikit learn. \n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1. Query the values of the feature we have chosen as target feature (class) and save them as numpy array\n",
    "2. Query the values of the features we want to use for training (_\"Fare\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\"_) and save them as numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "columns = [\"Fare\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\"]\n",
    "y = None # Exercise: Extract target feature \"Survived\" as 1D array\n",
    "x = None # Exercise: Extract the features we want to use for training as 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.25  ,  3.    ,  0.    , 22.    ,  1.    ],\n",
       "       [71.2833,  1.    ,  1.    , 38.    ,  1.    ],\n",
       "       [ 7.925 ,  3.    ,  1.    , 26.    ,  0.    ],\n",
       "       ...,\n",
       "       [23.45  ,  3.    ,  1.    ,     nan,  1.    ],\n",
       "       [30.    ,  1.    ,  0.    , 26.    ,  0.    ],\n",
       "       [ 7.75  ,  3.    ,  0.    , 32.    ,  0.    ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = targets = labels = train_df[\"Survived\"].values\n",
    "\n",
    "columns = [\"Fare\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\"]\n",
    "x = train_df[list(columns)].values\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "\n",
    "There are missing values (`NaN`) for some examples in the _\"Age\"_ column. Use the scikit learn `Imputer` class to replace them by the mean of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------First 5 with nan BEFORE----------\n",
      "[[ 8.4583  3.      0.         nan  0.    ]\n",
      " [13.      2.      0.         nan  0.    ]\n",
      " [ 7.225   3.      1.         nan  0.    ]\n",
      " [ 7.225   3.      0.         nan  0.    ]\n",
      " [ 7.8792  3.      1.         nan  0.    ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------First 5 with nan BEFORE----------\")\n",
    "nanMask = np.argwhere(np.isnan(x))\n",
    "print(x[nanMask[0:5,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# TODO as Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------First 5 with nan AFTER----------\n",
      "[[ 8.4583  3.      0.         nan  0.    ]\n",
      " [13.      2.      0.         nan  0.    ]\n",
      " [ 7.225   3.      1.         nan  0.    ]\n",
      " [ 7.225   3.      0.         nan  0.    ]\n",
      " [ 7.8792  3.      1.         nan  0.    ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------First 5 with nan AFTER----------\")\n",
    "print(x[nanMask[0:5,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------First 5 with nan BEFORE----------\n",
      "[[ 8.4583  3.      0.         nan  0.    ]\n",
      " [13.      2.      0.         nan  0.    ]\n",
      " [ 7.225   3.      1.         nan  0.    ]\n",
      " [ 7.225   3.      0.         nan  0.    ]\n",
      " [ 7.8792  3.      1.         nan  0.    ]]\n",
      "-----------First 5 with nan AFTER----------\n",
      "[[ 8.4583      3.          0.         29.69911765  0.        ]\n",
      " [13.          2.          0.         29.69911765  0.        ]\n",
      " [ 7.225       3.          1.         29.69911765  0.        ]\n",
      " [ 7.225       3.          0.         29.69911765  0.        ]\n",
      " [ 7.8792      3.          1.         29.69911765  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------First 5 with nan BEFORE----------\")\n",
    "nanMask = np.argwhere(np.isnan(x))\n",
    "print(x[nanMask[0:5,0]])\n",
    "\n",
    "imp = Imputer()#missing_values=nan, strategy='mean') #, axis=0)\n",
    "x = imp.fit_transform(x)\n",
    "\n",
    "print(\"-----------First 5 with nan AFTER----------\")\n",
    "print(x[nanMask[0:5,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define and learn a decision tree by the criterion _'entropy'_ and we restrict the depth of the tree to 3.\n",
    "We use the [scikit learn decison tree module](http://scikit-learn.org/stable/modules/tree.html).\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Define and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = None # Exercise: Define and train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "clf = clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert clf.criterion == \"entropy\"\n",
    "assert clf.max_depth == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clf` is an instance of a trained decision tree classifier.\n",
    "\n",
    "The decision tree can be visualized (after training). For this we must write an graphviz dot-File  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"982pt\" height=\"433pt\"\n",
       " viewBox=\"0.00 0.00 981.50 433.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 429)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-429 977.5,-429 977.5,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"#f5cfb4\" stroke=\"#000000\" points=\"544,-425 425,-425 425,-342 544,-342 544,-425\"/>\n",
       "<text text-anchor=\"middle\" x=\"484.5\" y=\"-409.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Sex &lt;= 0.5</text>\n",
       "<text text-anchor=\"middle\" x=\"484.5\" y=\"-394.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.961</text>\n",
       "<text text-anchor=\"middle\" x=\"484.5\" y=\"-379.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 891</text>\n",
       "<text text-anchor=\"middle\" x=\"484.5\" y=\"-364.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [549, 342]</text>\n",
       "<text text-anchor=\"middle\" x=\"484.5\" y=\"-349.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = died</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"#eb9e67\" stroke=\"#000000\" points=\"423,-306 304,-306 304,-223 423,-223 423,-306\"/>\n",
       "<text text-anchor=\"middle\" x=\"363.5\" y=\"-290.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Fare &lt;= 26.269</text>\n",
       "<text text-anchor=\"middle\" x=\"363.5\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.699</text>\n",
       "<text text-anchor=\"middle\" x=\"363.5\" y=\"-260.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 577</text>\n",
       "<text text-anchor=\"middle\" x=\"363.5\" y=\"-245.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [468, 109]</text>\n",
       "<text text-anchor=\"middle\" x=\"363.5\" y=\"-230.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = died</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M442.1801,-341.8796C432.8392,-332.6931 422.861,-322.8798 413.256,-313.4336\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"415.5845,-310.8146 406.0006,-306.2981 410.6762,-315.8054 415.5845,-310.8146\"/>\n",
       "<text text-anchor=\"middle\" x=\"406.2089\" y=\"-327.5972\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"#7ebfee\" stroke=\"#000000\" points=\"669.5,-306 557.5,-306 557.5,-223 669.5,-223 669.5,-306\"/>\n",
       "<text text-anchor=\"middle\" x=\"613.5\" y=\"-290.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Pclass &lt;= 2.5</text>\n",
       "<text text-anchor=\"middle\" x=\"613.5\" y=\"-275.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.824</text>\n",
       "<text text-anchor=\"middle\" x=\"613.5\" y=\"-260.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 314</text>\n",
       "<text text-anchor=\"middle\" x=\"613.5\" y=\"-245.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [81, 233]</text>\n",
       "<text text-anchor=\"middle\" x=\"613.5\" y=\"-230.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = survived</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>0&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M529.6179,-341.8796C539.674,-332.6031 550.423,-322.6874 560.7554,-313.1559\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"563.2124,-315.6511 568.1895,-306.2981 558.4661,-310.506 563.2124,-315.6511\"/>\n",
       "<text text-anchor=\"middle\" x=\"567.1822\" y=\"-327.5778\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"#e99457\" stroke=\"#000000\" points=\"235.5,-187 123.5,-187 123.5,-104 235.5,-104 235.5,-187\"/>\n",
       "<text text-anchor=\"middle\" x=\"179.5\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Age &lt;= 13.5</text>\n",
       "<text text-anchor=\"middle\" x=\"179.5\" y=\"-156.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.558</text>\n",
       "<text text-anchor=\"middle\" x=\"179.5\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 415</text>\n",
       "<text text-anchor=\"middle\" x=\"179.5\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [361, 54]</text>\n",
       "<text text-anchor=\"middle\" x=\"179.5\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = died</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M303.7605,-225.8641C284.6561,-213.5086 263.4267,-199.7787 243.9991,-187.2141\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"245.803,-184.2126 235.5053,-181.7208 242.0016,-190.0904 245.803,-184.2126\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"#f2c29f\" stroke=\"#000000\" points=\"419.5,-187 307.5,-187 307.5,-104 419.5,-104 419.5,-187\"/>\n",
       "<text text-anchor=\"middle\" x=\"363.5\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">SibSp &lt;= 2.5</text>\n",
       "<text text-anchor=\"middle\" x=\"363.5\" y=\"-156.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.924</text>\n",
       "<text text-anchor=\"middle\" x=\"363.5\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 162</text>\n",
       "<text text-anchor=\"middle\" x=\"363.5\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [107, 55]</text>\n",
       "<text text-anchor=\"middle\" x=\"363.5\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = died</text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>1&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M363.5,-222.8796C363.5,-214.6838 363.5,-205.9891 363.5,-197.5013\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"367.0001,-197.298 363.5,-187.2981 360.0001,-197.2981 367.0001,-197.298\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"#57ace9\" stroke=\"#000000\" points=\"105,-68 0,-68 0,0 105,0 105,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"52.5\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.567</text>\n",
       "<text text-anchor=\"middle\" x=\"52.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 15</text>\n",
       "<text text-anchor=\"middle\" x=\"52.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [2, 13]</text>\n",
       "<text text-anchor=\"middle\" x=\"52.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = survived</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M132.2099,-103.9815C121.4275,-94.5151 109.977,-84.462 99.2187,-75.0168\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"101.3507,-72.2311 91.5268,-68.2637 96.7324,-77.4915 101.3507,-72.2311\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"#e88f50\" stroke=\"#000000\" points=\"235.5,-68 123.5,-68 123.5,0 235.5,0 235.5,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"179.5\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.477</text>\n",
       "<text text-anchor=\"middle\" x=\"179.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 400</text>\n",
       "<text text-anchor=\"middle\" x=\"179.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [359, 41]</text>\n",
       "<text text-anchor=\"middle\" x=\"179.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = died</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M179.5,-103.9815C179.5,-95.618 179.5,-86.7965 179.5,-78.3409\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"183.0001,-78.2636 179.5,-68.2637 176.0001,-78.2637 183.0001,-78.2636\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"#f6d1b7\" stroke=\"#000000\" points=\"359,-68 254,-68 254,0 359,0 359,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"306.5\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.964</text>\n",
       "<text text-anchor=\"middle\" x=\"306.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 139</text>\n",
       "<text text-anchor=\"middle\" x=\"306.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [85, 54]</text>\n",
       "<text text-anchor=\"middle\" x=\"306.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = died</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M342.2753,-103.9815C337.8119,-95.2504 333.0933,-86.0202 328.5987,-77.2281\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"331.6842,-75.5745 324.016,-68.2637 325.4514,-78.7608 331.6842,-75.5745\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"#e68742\" stroke=\"#000000\" points=\"481.5,-68 377.5,-68 377.5,0 481.5,0 481.5,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"429.5\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.258</text>\n",
       "<text text-anchor=\"middle\" x=\"429.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 23</text>\n",
       "<text text-anchor=\"middle\" x=\"429.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [22, 1]</text>\n",
       "<text text-anchor=\"middle\" x=\"429.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = died</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>5&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M388.076,-103.9815C393.2986,-95.1585 398.8229,-85.8258 404.0763,-76.9506\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"407.1364,-78.652 409.2184,-68.2637 401.1126,-75.0863 407.1364,-78.652\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<polygon fill=\"#44a2e6\" stroke=\"#000000\" points=\"666,-187 561,-187 561,-104 666,-104 666,-187\"/>\n",
       "<text text-anchor=\"middle\" x=\"613.5\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Fare &lt;= 28.856</text>\n",
       "<text text-anchor=\"middle\" x=\"613.5\" y=\"-156.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.299</text>\n",
       "<text text-anchor=\"middle\" x=\"613.5\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 170</text>\n",
       "<text text-anchor=\"middle\" x=\"613.5\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [9, 161]</text>\n",
       "<text text-anchor=\"middle\" x=\"613.5\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = survived</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M613.5,-222.8796C613.5,-214.6838 613.5,-205.9891 613.5,-197.5013\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"617.0001,-197.298 613.5,-187.2981 610.0001,-197.2981 617.0001,-197.298\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"#000000\" points=\"851,-187 746,-187 746,-104 851,-104 851,-187\"/>\n",
       "<text text-anchor=\"middle\" x=\"798.5\" y=\"-171.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Fare &lt;= 23.35</text>\n",
       "<text text-anchor=\"middle\" x=\"798.5\" y=\"-156.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 1.0</text>\n",
       "<text text-anchor=\"middle\" x=\"798.5\" y=\"-141.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 144</text>\n",
       "<text text-anchor=\"middle\" x=\"798.5\" y=\"-126.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [72, 72]</text>\n",
       "<text text-anchor=\"middle\" x=\"798.5\" y=\"-111.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = died</text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>8&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M669.5081,-228.4731C690.914,-214.7039 715.425,-198.9374 737.2915,-184.872\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"739.3292,-187.7228 745.8461,-179.3693 735.5423,-181.8356 739.3292,-187.7228\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<polygon fill=\"#4fa8e8\" stroke=\"#000000\" points=\"605,-68 500,-68 500,0 605,0 605,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"552.5\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.469</text>\n",
       "<text text-anchor=\"middle\" x=\"552.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 70</text>\n",
       "<text text-anchor=\"middle\" x=\"552.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [7, 63]</text>\n",
       "<text text-anchor=\"middle\" x=\"552.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = survived</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M590.7859,-103.9815C586.0092,-95.2504 580.9595,-86.0202 576.1494,-77.2281\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"579.1153,-75.3568 571.2451,-68.2637 572.9742,-78.7165 579.1153,-75.3568\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<polygon fill=\"#3d9fe6\" stroke=\"#000000\" points=\"728,-68 623,-68 623,0 728,0 728,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"675.5\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.141</text>\n",
       "<text text-anchor=\"middle\" x=\"675.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 100</text>\n",
       "<text text-anchor=\"middle\" x=\"675.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [2, 98]</text>\n",
       "<text text-anchor=\"middle\" x=\"675.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = survived</text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>9&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M636.5865,-103.9815C641.4415,-95.2504 646.574,-86.0202 651.4629,-77.2281\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"654.6467,-78.7043 656.4476,-68.2637 648.5288,-75.3025 654.6467,-78.7043\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<polygon fill=\"#c3e1f7\" stroke=\"#000000\" points=\"851,-68 746,-68 746,0 851,0 851,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"798.5\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.977</text>\n",
       "<text text-anchor=\"middle\" x=\"798.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 117</text>\n",
       "<text text-anchor=\"middle\" x=\"798.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [48, 69]</text>\n",
       "<text text-anchor=\"middle\" x=\"798.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = survived</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M798.5,-103.9815C798.5,-95.618 798.5,-86.7965 798.5,-78.3409\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"802.0001,-78.2636 798.5,-68.2637 795.0001,-78.2637 802.0001,-78.2636\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<polygon fill=\"#e89152\" stroke=\"#000000\" points=\"973.5,-68 869.5,-68 869.5,0 973.5,0 973.5,-68\"/>\n",
       "<text text-anchor=\"middle\" x=\"921.5\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">entropy = 0.503</text>\n",
       "<text text-anchor=\"middle\" x=\"921.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">samples = 27</text>\n",
       "<text text-anchor=\"middle\" x=\"921.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">value = [24, 3]</text>\n",
       "<text text-anchor=\"middle\" x=\"921.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">class = died</text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>12&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M844.3007,-103.9815C854.7434,-94.5151 865.8333,-84.462 876.2528,-75.0168\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"878.6441,-77.5731 883.7024,-68.2637 873.9428,-72.3868 878.6441,-77.5731\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.files.Source at 0x7f730c9c4a30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = Source(tree.export_graphviz(clf, out_file=None\n",
    "   , feature_names=columns, class_names=['died', 'survived'] \n",
    "   , filled = True))\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dtree_render.png'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To open in seperate window and save it\n",
    "graph.format = 'png'\n",
    "graph.render('dtree_render',view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the decision tree the main criterion (root node) for survival is the sex of the passenger (_if your implementation is correct so far_). In the left subtree are the male passengers (sex = 0), in the right subtree the female (sex=1). \n",
    "\n",
    "In the leafs the class information is given by a `value` array. Here the second value is the number of survivors in the leaves.\n",
    "\n",
    "For example the leftmost leaf represents passengers that are male (sex=0) with fare<=26.2687 and age<=13.5. 13 of such boys survived and 2 of them died.\n",
    "\n",
    "The entropy $- \\sum p_i \\log_2 (p_i)$ is displayed also at each node (splitting criterion).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction / Validation\n",
    "\n",
    "To make predictions with scikit learn, we must convert the data in\n",
    "the same way as we did it with the training data. Then we could use the method:    \n",
    "\n",
    "    clf.predict(validation_data)\n",
    "    \n",
    "The depth of the decision tree is a hyperparameter. So the depth should be determined with the help of a \n",
    "validation set or by cross validation.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. Manually split your training data in 75% for training and 25% for validation\n",
    "2. Repeat the training process several times for different tree depths (define a new model every time)\n",
    "3. Use `clf.predict(validation_data)` to predict your validation data\n",
    "3. Manually calculate the accuracy for the validation set each time\n",
    " - Accuracy is defined as $\\frac{\\text{number of correct predictions}}{\\text{number of all predictions}}$\n",
    "4. From your results: Which seems to be the best tree depths?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Your code below to divide x and y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668\n"
     ]
    }
   ],
   "source": [
    "length = int(len(x) * 0.75)\n",
    "print(length)\n",
    "x_train = x[:length]\n",
    "x_val = x[length:]\n",
    "y_train = y[:length]\n",
    "y_val = y[length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Your code below to train and predict trees with different depths\n",
    "\n",
    "for depth in range(1,20):\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:  1  acc:  0.789237668161435\n",
      "depth:  2  acc:  0.789237668161435\n",
      "depth:  3  acc:  0.8385650224215246\n",
      "depth:  4  acc:  0.8340807174887892\n",
      "depth:  5  acc:  0.8340807174887892\n",
      "depth:  6  acc:  0.8340807174887892\n",
      "depth:  7  acc:  0.8340807174887892\n",
      "depth:  8  acc:  0.8161434977578476\n",
      "depth:  9  acc:  0.8295964125560538\n",
      "depth:  10  acc:  0.8295964125560538\n",
      "depth:  11  acc:  0.8340807174887892\n",
      "depth:  12  acc:  0.8340807174887892\n",
      "depth:  13  acc:  0.8251121076233183\n",
      "depth:  14  acc:  0.8116591928251121\n",
      "depth:  15  acc:  0.8161434977578476\n",
      "depth:  16  acc:  0.820627802690583\n",
      "depth:  17  acc:  0.8026905829596412\n",
      "depth:  18  acc:  0.7982062780269058\n",
      "depth:  19  acc:  0.8026905829596412\n"
     ]
    }
   ],
   "source": [
    "for depth in range(1,20):\n",
    "    clf = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=depth)\n",
    "    clf = clf.fit(x_train, y_train)\n",
    "    preds = clf.predict(x_val)\n",
    "    acc = (preds - y_val) \n",
    "    acc = [1 if i == -1 else i for i in acc]\n",
    "    acc = 1 - np.sum(acc) / len(y_val)\n",
    "    print('depth: ', depth, ' acc: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossvalidation\n",
    "\n",
    "Now with a dataset with less than 900 data examples (we have 891), chances to unluckily divide training and validation split seem pretty high. Also \"wasting\" 25% for validation on such a small data set is also not the ideal solution.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Now do not manually divide your data and do not manually compute the accuracy. Instead use the function `sklearn.model_selection.cross_val_score` to search the best tree depth. Use 10-fold crossvalidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Your code below for crossvalidation\n",
    "\n",
    "for depth in range(1,20):\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:  1  acc:  0.786729088639201\n",
      "depth:  2  acc:  0.7688764044943821\n",
      "depth:  3  acc:  0.8170536828963796\n",
      "depth:  4  acc:  0.8070786516853932\n",
      "depth:  5  acc:  0.8205368289637953\n",
      "depth:  6  acc:  0.8216354556803995\n",
      "depth:  7  acc:  0.8182771535580524\n",
      "depth:  8  acc:  0.8116104868913858\n",
      "depth:  9  acc:  0.8204993757802747\n",
      "depth:  10  acc:  0.8148938826466916\n",
      "depth:  11  acc:  0.818214731585518\n",
      "depth:  12  acc:  0.804769038701623\n",
      "depth:  13  acc:  0.8081523096129837\n",
      "depth:  14  acc:  0.7980898876404494\n",
      "depth:  15  acc:  0.7958551810237202\n",
      "depth:  16  acc:  0.7991885143570536\n",
      "depth:  17  acc:  0.7890511860174781\n",
      "depth:  18  acc:  0.7913483146067416\n",
      "depth:  19  acc:  0.7857303370786517\n"
     ]
    }
   ],
   "source": [
    "depths = []\n",
    "for i in range(1,20):\n",
    "    clf = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=i)\n",
    "    scores = cross_val_score(estimator=clf, X=x, y=y, cv=10, n_jobs=4, scoring=\"accuracy\")\n",
    "    depths.append((i, scores.mean()))\n",
    "    \n",
    "for d in depths: print('depth: ', d[0], ' acc: ', d[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch\n",
    "\n",
    "This already works pretty well to determine one hyperparameter. But imagine you had several hyperparameters. Since the computation (even with 10 fold crossvalidation) is very fast for decision trees, it is no problem at all to try out every possible combination of hyperparameters. For each hyperparameter you would have to implement another for-loop. Should be no problem. But for convenience, you should know scikit-learn also has a built-in function for that.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "Use `sklearn.model_selection.GridSearchCV` to do a grid search for the best hyperparameters. In addition to `max_depth` parameter between _[1_, _20]_, add the `criterion` as another hyper parameter with the possible values _\"entropy\"_ and _\"gini\"_.\n",
    "\n",
    "Further useful functions after the grid search: `clf.best_score_` and `clf.best_params_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Your code below for grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8227590511860174 {'criterion': 'entropy', 'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'max_depth':range(1,20), 'criterion':['entropy','gini']}\n",
    "clf = GridSearchCV(tree.DecisionTreeClassifier(), parameters, n_jobs=4, cv=10)\n",
    "clf.fit(X=x, y=y)\n",
    "tree_model = clf.best_estimator_\n",
    "print (clf.best_score_, clf.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Criterion Entropy / Information Gain\n",
    "\n",
    "As you might have noticed, packages like _scikit learn_ can be very comfortable, since they do most of the work for you as long as you provide the data in the right format. Though a good advice is, that you should know what happens inside of such a black box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:**\n",
    "\n",
    "Recompute the root node entropy. On Pen & paper or with just some lines of code here using basic mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9607079018756469"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_survived = y.sum()\n",
    "p_survived = float(y.sum())/len(y)\n",
    "p_not_survived = 1. - p_survived\n",
    "entropy_before_split  = - p_survived * np.log2(p_survived) - p_not_survived * np.log2(p_not_survived)\n",
    "entropy_before_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(entropy_before_split, 0.9607079018756469, decimal=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:**\n",
    "\n",
    "Compute the information gain of the first split node (root node). Use the shown entropy values and number of data records (samples). Again on Pen & paper or with just some lines of code here using basic mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy left node: 0.6991817891208407\n",
      "entropy right node: 0.8236550739295191\n",
      "Information gain of the root node split: 0.2176601066606142\n"
     ]
    }
   ],
   "source": [
    "entropy_left = -(468/577 * np.log2(468/577) + 109/577 * np.log2(109/577))\n",
    "entropy_right = -(81/314 * np.log2(81/314) + 233/314 * np.log2(233/314))\n",
    "print(\"entropy left node:\", entropy_left)\n",
    "print(\"entropy right node:\", entropy_right)\n",
    "weighted_entropy_after_split = 577./(577+314) * entropy_left + 314./(577+314) * entropy_right\n",
    "information_gain_split_root_node = entropy_before_split - weighted_entropy_after_split \n",
    "print(\"Information gain of the root node split:\", information_gain_split_root_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(information_gain_split_root_node, 0.2176601066606142, decimal=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:**\n",
    "\n",
    "Compute the information gain of the following split table:\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "    <tr>\n",
    "        <th>\n",
    "        </th>\n",
    "        <th>\n",
    "            class 0\n",
    "        </th>\n",
    "        <th>\n",
    "            class 1\n",
    "        </th>\n",
    "    </tr>\n",
    "    </thead>\n",
    "    <tr>\n",
    "        <td>\n",
    "            feature $\\le$ v\n",
    "        </td>\n",
    "        <td>\n",
    "            2\n",
    "        </td>\n",
    "        <td>\n",
    "            13\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            feature $>$ v\n",
    "        </td>\n",
    "        <td>\n",
    "            359\n",
    "        </td>\n",
    "        <td>\n",
    "            41\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    " \n",
    "\n",
    "The numbers are the corresponding data records, e.g. there are 13 data records with target class 1 and feature $\\le$ v. \n",
    "\n",
    "Write a python function that computes the information gain.The data is given by a python array:\n",
    "\n",
    "Like always, you are free to write as many helper functions as you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[2.,13.],[359., 41.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(data):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "### Solution\n",
    "### Very explicit and special:\n",
    "\n",
    "def entropy(p_):\n",
    "    p = p_.copy()\n",
    "    p[p != 0] = - p[p != 0] * np.log2(p[p != 0] )\n",
    "    return p.sum()\n",
    "\n",
    "def information_gain(data):\n",
    "    entropy_feature_smaller_v = entropy(data[0]/data[0].sum())\n",
    "    entropy_feature_bigger_v = entropy(data[1]/data[1].sum())\n",
    "    #print(entropy_feature_smaller_v)\n",
    "    #print(entropy_feature_bigger_v)\n",
    "    entropy_before_split = entropy(data.sum(axis=0)/data.sum(axis=0).sum())\n",
    "    weights = data.sum(axis=1)/data.sum()\n",
    "    weighted_entropy_after_split = weights[0] * entropy_feature_smaller_v + weights[1] * entropy_feature_bigger_v\n",
    "    return entropy_before_split - weighted_entropy_after_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain: 0.07765787804281093\n"
     ]
    }
   ],
   "source": [
    "np.testing.assert_almost_equal(information_gain(data), 0.07765787804281093)\n",
    "print(\"Information Gain:\", information_gain(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "More general and reusable solution based on a probability table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy H(Y) =  0.5577685146123689\n",
      "Conditional-Entropy H(Y|X) =  0.480110636569558\n",
      "Information Gain I(Y; X) = H(Y) - H(Y|X) = 0.07765787804281088\n"
     ]
    }
   ],
   "source": [
    "def p_log_p(p_):\n",
    "    p = p_.copy()\n",
    "    p[p != 0] = - p[p != 0] * np.log2(p[p != 0] )\n",
    "    return p\n",
    "\n",
    "def entropy(p):\n",
    "    assert (p>=0.).all()\n",
    "    assert np.allclose(1., p.sum(), atol=1e-08)\n",
    "    return p_log_p(p).sum()\n",
    "\n",
    "def conditional_entropy(p, axis=0):\n",
    "    if axis == 1:\n",
    "        p = p.T\n",
    "    p_y_given_x = p / p.sum(axis=0)\n",
    "    c = p_log_p(p_y_given_x) \n",
    "    p_x = p.sum(axis=0)/p.sum()\n",
    "    s = c * p_x\n",
    "    return s.sum()\n",
    " \n",
    "def information_gain(p0, axis = 0):\n",
    "    p = p0.copy()\n",
    "    if axis == 1:\n",
    "        p = p.T\n",
    "    p_ = p.sum(axis=1)\n",
    "    return entropy(p_) - conditional_entropy(p)\n",
    "\n",
    "p = data / data.sum()\n",
    "p = p.T\n",
    "p_y = p.sum(axis=1)\n",
    "\n",
    "print(\"Entropy H(Y) = \", entropy(p_y))\n",
    "print(\"Conditional-Entropy H(Y|X) = \",  conditional_entropy(p))\n",
    "print(\"Information Gain I(Y; X) = H(Y) - H(Y|X) =\", information_gain(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Summary and Outlook\n",
    "\n",
    "Machine learning is not exclusively about neural networks - In this notebook you implemented a decision tree, which chains a set of if-then-else decision rules to make a prediction. You employed two popular APIs, scikit-learn and pandas. You also calculated entropy and information gain, measures that are widely applicable in a host of statistics tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <a name=\"CRI\"></a>[CRI]\n",
    "        </td>\n",
    "        <td>\n",
    "            A. Criminisi, J. Shotton, and E. Konukoglu, Decision Forests for Classification, Regression, Density Estimation, Manifold Learning and Semi- Supervised Learning, no. MSR-TR-2011-114, 28 October 2011. - http://research.microsoft.com/pubs/155552/decisionForests_MSR_TR_2011_114.pdf\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Licenses\n",
    "\n",
    "### Notebook License (CC-BY-SA 4.0)\n",
    "\n",
    "*The following license applies to the complete notebook, including code cells. It does however not apply to any referenced external media (e.g., images).*\n",
    "\n",
    "Exercise: Logistic Regression and Regularization <br/>\n",
    "by Christian Herta, Klaus Strohmenger<br/>\n",
    "is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).<br/>\n",
    "Based on a work at https://gitlab.com/deep.TEACHING.\n",
    "\n",
    "\n",
    "### Code License (MIT)\n",
    "\n",
    "*The following license only applies to code cells of the notebook.*\n",
    "\n",
    "Copyright 2018 Christian Herta, Klaus Strohmenger\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
