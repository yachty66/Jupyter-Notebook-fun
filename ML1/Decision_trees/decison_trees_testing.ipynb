{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2f3a3d4",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this exercise you will train and evaluate a decision tree. Opposed to the previous exercises with the topics _univariate linear regression_, _multivariate linear regression_, _logistic regression_ and _bias variance tradeoff_, you will not implement the algorithms from scratch using numpy. Instead you will use two python packages written on top of numpy, namely the _pandas_ and the _scikit-learn_ package, which are both widely used by the machine learning community. The steps can be broken down into:\n",
    "\n",
    "1. Load the data from a _csv file_ into a pandas DataFrame object\n",
    "2. Preprocess the data\n",
    "3. Train a decision tree and visualize it.\n",
    "4. Do hyperparameter optimization by dividing the dataset into a fixed training set and a fixed validation set.\n",
    "5. Do hyperparameter optimization by crossvalidation\n",
    "6. Do hyperparameter optimization by gridsearch\n",
    "\n",
    "Afterwards, show that you understand, what computations are done by the _scikit-learn_ package you have used by manually computing:\n",
    "\n",
    "7. The entropy for one node\n",
    "8. The information gain for one node\n",
    "\n",
    "**Note:**\n",
    "\n",
    "As this exercise heavily focuses on the usage of the high-level APIs _pandas_ and _scikit-learn_, reading their documentations (just the parts corresponding to the current task) is strongly recommended:\n",
    "\n",
    "- https://scikit-learn.org/stable/documentation.html#\n",
    "- https://pandas.pydata.org/pandas-docs/stable/search.html?q=&check_keywords=yes&area=default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b01522b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0n/n106l5rn7cb6yw8038g17vcm0000gn/T/ipykernel_26451/1721495102.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgraphviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#from sklearn.preprocessing import Imputer # in newer versions: \n",
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "#old: from sklearn.externals.six import StringIO\n",
    "from six import StringIO\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from graphviz import Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e752fd0",
   "metadata": {},
   "source": [
    "## Teaching Content\n",
    "\n",
    "### Decision Trees for Classification\n",
    "\n",
    "In a decision tree the data is split at each node according to a decision rule. This corresponds to nested *if-then-else*-rules. In the *if*-part of such a rule are decision is made based on a feature of the data record. \n",
    "\n",
    "We will use the scikit learn implementation. For this implementation the features must be binary or have (at least) ordinal characteristic. If a feature is e.g. nominal with many values, it must be converted to a set of binary (one-hot-coded) features.\n",
    "\n",
    "\n",
    "The splitting rules in the scikit learn implementation are binary and are based on a threshold, e.g.\n",
    "  - _if $x_6 <= 2.14$ then_ left subbranch, *else* right subbranch.         \n",
    "  - binary features must be coded as 0/1, so the rule becomes: if $x_2 <= 0.5$ _then_ left subbranch, _else_ right subbranch. \n",
    "\n",
    "\n",
    "<!--The structure of the tree will be determined by data (see below) and a training procedure.-->\n",
    "\n",
    "In the leaves of the tree the (class) predictions are made. There are two possibilities for such an inference: \n",
    "   - hard assignment: Predict for the data records which end up on a leaf by the majority class of the training data that end up on that leaf.          \n",
    "   - soft assignment: Assign probabilities according to the distribution of the classes in respect to the training data which end up on that leaf.\n",
    "\n",
    "As an example of a decision tree we will learn the following tree from the titanic data set: \n",
    "\n",
    "<img src=\"https://gitlab.com/deep.TEACHING/educational-materials/raw/master/media/klaus/exercise-decision-trees-a-tree.png\" width=\"1024\" alt=\"internet connection needed\">\n",
    "\n",
    "A full explanation of the tree will be given later. Here just look at the decision rules (first line of the inner nodes) and at the last line of the leafs. In each leaf you see an array (values) with counts of the different targets for the train data: [number_died, number_survivors] .\n",
    "\n",
    "### Learning \n",
    "\n",
    "Finding a tree that splits the training data optimal is [np-hard](https://en.wikipedia.org/wiki/NP-hardness). Therefore often a *greedy*-strategy is used:\n",
    "\n",
    "To build up a decision tree the algorithm starts at the root of the tree. The feature and the threshold \n",
    "that splits the training data best (with respect to the classes) are chosen. In an iterative way the whole tree is build up by such splitting rules. \n",
    "\n",
    "There are different criteria for measuring the \"separation (split) quality\". The most important ones are:\n",
    "\n",
    "- Gini Impurity \n",
    "- Information Gain \n",
    "\n",
    "In this tutorial we concentrate on the information gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ea75ef",
   "metadata": {},
   "source": [
    "### Information Gain as Splitting Criterion\n",
    "\n",
    "The entropy with respect to the target class variable $y$ of a training data set $\\mathcal D$ is defined as:\n",
    "\n",
    "$$\n",
    " H(y, \\mathcal D) = - \\sum_{y \\in \\mathcal Y} p(y|\\mathcal D) \\log_2 p(y|\\mathcal D)\n",
    "$$\n",
    "with the domain of the target values $\\mathcal Y = \\{t_1, t_2,... \\}$.\n",
    "\n",
    "\n",
    "The probabilities are estimated by \n",
    "$$\n",
    "  p(y=t_i, \\mathcal D) = |\\mathcal D^{(y=t_i)}| /|\\mathcal D| \n",
    "$$    \n",
    "\n",
    "\n",
    "with the number of training data $|\\mathcal D|$  and the number of training data $|\\mathcal D^{(y=t_i)}|$ with target label $t_i$: \n",
    "\n",
    "\n",
    "On a node a (binary) split on a feature $x_k$ is made by the split rule $x_k \\leq v$. \n",
    "As result there are two data sets $\\mathcal D_0$ and $\\mathcal D_1$ for the left resp. the right branch.\n",
    "\n",
    "The feature $x_k$ and the split value $v$ are chosen that they maximize the 'reduction of the entropy' measured by the information gain $I$:\n",
    "$$\n",
    "  I(y; x_k) = H(y, \\mathcal D) - H(y|x_k) = H(y, \\mathcal D) - \\sum_{j=0}^1 p_jH(y, \\mathcal D_j) =\n",
    "  H(y, \\mathcal D) + \\sum_{j=0}^1  \\sum_{y \\in \\mathcal Y} \\frac{|\\mathcal D_j|}{|\\mathcal D|} p(y|\\mathcal D_j) \\log_2 p(y|\\mathcal D_j)\n",
    "$$\n",
    "Note that $p_{j=0}$  is the estimated probability that a random data record of $\\mathcal D$ has feature value $x_k \\leq v$ which can be estimated by ${|\\mathcal D_0|}/{|\\mathcal D|}$ (analog for $j=1$).\n",
    "\n",
    "$p(y=t_i|\\mathcal D_0)$ can also be estimated by the fraction of the counts ${|\\mathcal D_0^{(y=t_i)}|}/{|\\mathcal D_0|}$. \n",
    "So the information gain can be computed just with counts:\n",
    "\n",
    "\n",
    "$$\n",
    "  I(y; x_k) = -\n",
    "   \\sum_{y \\in \\mathcal Y} \\frac{|\\mathcal D^{(y=t_i)}|}{|\\mathcal D|}  \\log_2 \\frac{|\\mathcal D^{(y=t_i)}|}{|\\mathcal D|} + \\sum_{j=0}^1  \\sum_{y \\in \\mathcal Y} \\frac{|\\mathcal D_j^{(y=t_i)}|}{|\\mathcal D|}  \\log_2 \\frac{|\\mathcal D_j^{(y=t_i)}|}{|\\mathcal D_j|}\n",
    "$$\n",
    "\n",
    "\n",
    "<!--$|\\mathcal D_0|$ respectivly $|\\mathcal D_1|$ is the number of elements in the splitted data sets.-->\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f728dbf",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "Deep decision trees generalize often poorly. The following remedies reduce overfitting: \n",
    "\n",
    "- Limitation of the maximal depth of the tree. \n",
    "- Pruning with an validation set either during training (pre-pruning) or after training (post-pruning).\n",
    "- Dimensionality reduction (reducing the number of features before training)\n",
    "\n",
    "\n",
    "Also often combining decision trees to an ensemble (decision forests) is used against overfitting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772ac15",
   "metadata": {},
   "source": [
    "### Loading the Data\n",
    "\n",
    "First we read in the _titanic data set_ with _pandas_\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1. Complete the code to load the dataset as [_pandas_](http://pandas.pydata.org/) [DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html) object.\n",
    "2. Either download the [_titanic-train.csv_](https://gitlab.com/deep.TEACHING/educational-materials/raw/master/datasets/titanic-train.csv) or alternatively read the url directly into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca528c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise: Load the csv file as pandas DataFrame\n",
    "url = 'https://gitlab.com/deep.TEACHING/educational-materials/raw/master/notebooks/data/titanic-train.csv'\n",
    "\n",
    "train_df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee75f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(891, 12)\n"
     ]
    }
   ],
   "source": [
    "### Not an exercise, just execute the cell\n",
    "\n",
    "print(train_df.ndim)\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07cb79e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf8231f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        male\n",
       "1      female\n",
       "2      female\n",
       "3      female\n",
       "4        male\n",
       "        ...  \n",
       "886      male\n",
       "887    female\n",
       "888    female\n",
       "889      male\n",
       "890      male\n",
       "Name: Sex, Length: 891, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"Sex\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0f84e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Sex\"] = train_df[\"Sex\"].apply(lambda sex: 1 if sex == 'female' else 0)\n",
    "# or\n",
    "#df.Sex[df[\"Sex\"]=='female'] = 1\n",
    "#df.Sex[df[\"Sex\"]=='male'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7089d7e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.25  ,  3.    ,  0.    , 22.    ,  1.    ],\n",
       "       [71.2833,  1.    ,  1.    , 38.    ,  1.    ],\n",
       "       [ 7.925 ,  3.    ,  1.    , 26.    ,  0.    ],\n",
       "       ...,\n",
       "       [23.45  ,  3.    ,  1.    ,     nan,  1.    ],\n",
       "       [30.    ,  1.    ,  0.    , 26.    ,  0.    ],\n",
       "       [ 7.75  ,  3.    ,  0.    , 32.    ,  0.    ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = targets = labels = train_df[\"Survived\"].values\n",
    "\n",
    "columns = [\"Fare\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\"]\n",
    "x = train_df[list(columns)].values\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9ec927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------First 5 with nan BEFORE----------\n",
      "[[ 8.4583  3.      0.         nan  0.    ]\n",
      " [13.      2.      0.         nan  0.    ]\n",
      " [ 7.225   3.      1.         nan  0.    ]\n",
      " [ 7.225   3.      0.         nan  0.    ]\n",
      " [ 7.8792  3.      1.         nan  0.    ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------First 5 with nan BEFORE----------\")\n",
    "nanMask = np.argwhere(np.isnan(x))\n",
    "print(x[nanMask[0:5,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b1add02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------First 5 with nan AFTER----------\n",
      "[[ 8.4583  3.      0.         nan  0.    ]\n",
      " [13.      2.      0.         nan  0.    ]\n",
      " [ 7.225   3.      1.         nan  0.    ]\n",
      " [ 7.225   3.      0.         nan  0.    ]\n",
      " [ 7.8792  3.      1.         nan  0.    ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------First 5 with nan AFTER----------\")\n",
    "print(x[nanMask[0:5,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f53e75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------First 5 with nan BEFORE----------\n",
      "[[ 8.4583  3.      0.         nan  0.    ]\n",
      " [13.      2.      0.         nan  0.    ]\n",
      " [ 7.225   3.      1.         nan  0.    ]\n",
      " [ 7.225   3.      0.         nan  0.    ]\n",
      " [ 7.8792  3.      1.         nan  0.    ]]\n",
      "-----------First 5 with nan AFTER----------\n",
      "[[ 8.4583      3.          0.         29.69911765  0.        ]\n",
      " [13.          2.          0.         29.69911765  0.        ]\n",
      " [ 7.225       3.          1.         29.69911765  0.        ]\n",
      " [ 7.225       3.          0.         29.69911765  0.        ]\n",
      " [ 7.8792      3.          1.         29.69911765  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------First 5 with nan BEFORE----------\")\n",
    "nanMask = np.argwhere(np.isnan(x))\n",
    "print(x[nanMask[0:5,0]])\n",
    "\n",
    "imp = Imputer()#missing_values=nan, strategy='mean') #, axis=0)\n",
    "x = imp.fit_transform(x)\n",
    "\n",
    "print(\"-----------First 5 with nan AFTER----------\")\n",
    "print(x[nanMask[0:5,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "566de6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "clf = clf.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0521766",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Source' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0n/n106l5rn7cb6yw8038g17vcm0000gn/T/ipykernel_26451/3624787322.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m graph = Source(tree.export_graphviz(clf, out_file=None\n\u001b[0m\u001b[1;32m      2\u001b[0m    \u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'died'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'survived'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m    , filled = True))\n\u001b[1;32m      4\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Source' is not defined"
     ]
    }
   ],
   "source": [
    "graph = Source(tree.export_graphviz(clf, out_file=None\n",
    "   , feature_names=columns, class_names=['died', 'survived'] \n",
    "   , filled = True))\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c03c4e82",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0n/n106l5rn7cb6yw8038g17vcm0000gn/T/ipykernel_26451/2175503814.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### To open in seperate window and save it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dtree_render'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "### To open in seperate window and save it\n",
    "graph.format = 'png'\n",
    "graph.render('dtree_render',view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52624a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668\n"
     ]
    }
   ],
   "source": [
    "length = int(len(x) * 0.75)\n",
    "print(length)\n",
    "x_train = x[:length]\n",
    "x_val = x[length:]\n",
    "y_train = y[:length]\n",
    "y_val = y[length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52a11c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:  1  acc:  0.789237668161435\n",
      "depth:  2  acc:  0.789237668161435\n",
      "depth:  3  acc:  0.8385650224215246\n",
      "depth:  4  acc:  0.8340807174887892\n",
      "depth:  5  acc:  0.8340807174887892\n",
      "depth:  6  acc:  0.8340807174887892\n",
      "depth:  7  acc:  0.8385650224215246\n",
      "depth:  8  acc:  0.8161434977578476\n",
      "depth:  9  acc:  0.8295964125560538\n",
      "depth:  10  acc:  0.8251121076233183\n",
      "depth:  11  acc:  0.8340807174887892\n",
      "depth:  12  acc:  0.8340807174887892\n",
      "depth:  13  acc:  0.8161434977578476\n",
      "depth:  14  acc:  0.8116591928251121\n",
      "depth:  15  acc:  0.8116591928251121\n",
      "depth:  16  acc:  0.820627802690583\n",
      "depth:  17  acc:  0.8071748878923767\n",
      "depth:  18  acc:  0.8026905829596412\n",
      "depth:  19  acc:  0.8026905829596412\n"
     ]
    }
   ],
   "source": [
    "for depth in range(1,20):\n",
    "    clf = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=depth)\n",
    "    clf = clf.fit(x_train, y_train)\n",
    "    preds = clf.predict(x_val)\n",
    "    acc = (preds - y_val) \n",
    "    acc = [1 if i == -1 else i for i in acc]\n",
    "    acc = 1 - np.sum(acc) / len(y_val)\n",
    "    print('depth: ', depth, ' acc: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f29764d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth:  1  acc:  0.786729088639201\n",
      "depth:  2  acc:  0.7688764044943821\n",
      "depth:  3  acc:  0.8170536828963796\n",
      "depth:  4  acc:  0.8081897627965045\n",
      "depth:  5  acc:  0.8227590511860174\n",
      "depth:  6  acc:  0.8182896379525593\n",
      "depth:  7  acc:  0.8194007490636703\n",
      "depth:  8  acc:  0.8149812734082398\n",
      "depth:  9  acc:  0.8182771535580524\n",
      "depth:  10  acc:  0.8137578027465668\n",
      "depth:  11  acc:  0.8171161048689137\n",
      "depth:  12  acc:  0.802521847690387\n",
      "depth:  13  acc:  0.8148689138576779\n",
      "depth:  14  acc:  0.7980649188514357\n",
      "depth:  15  acc:  0.8036953807740325\n",
      "depth:  16  acc:  0.8014357053682897\n",
      "depth:  17  acc:  0.7991760299625469\n",
      "depth:  18  acc:  0.7868539325842696\n",
      "depth:  19  acc:  0.786816479400749\n"
     ]
    }
   ],
   "source": [
    "depths = []\n",
    "for i in range(1,20):\n",
    "    clf = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=i)\n",
    "    scores = cross_val_score(estimator=clf, X=x, y=y, cv=10, n_jobs=4, scoring=\"accuracy\")\n",
    "    depths.append((i, scores.mean()))\n",
    "    \n",
    "for d in depths: print('depth: ', d[0], ' acc: ', d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "996627cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8227590511860174 {'criterion': 'entropy', 'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'max_depth':range(1,20), 'criterion':['entropy','gini']}\n",
    "clf = GridSearchCV(tree.DecisionTreeClassifier(), parameters, n_jobs=4, cv=10)\n",
    "clf.fit(X=x, y=y)\n",
    "tree_model = clf.best_estimator_\n",
    "print (clf.best_score_, clf.best_params_) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8fe29e",
   "metadata": {},
   "source": [
    "The entropy with respect to the target class variable $y$ of a training data set $\\mathcal D$ is defined as:\n",
    "\n",
    "$$\n",
    " H(y, \\mathcal D) = - \\sum_{y \\in \\mathcal Y} p(y|\\mathcal D) \\log_2 p(y|\\mathcal D)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0031448d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "-0.0\n"
     ]
    }
   ],
   "source": [
    "nb_survived = y.sum()\n",
    "p_survived = float(y.sum())/len(y)\n",
    "p_not_survived = 1. - p_survived\n",
    "entropy_before_split = -p_survived * np.log2(p_survived) - p_not_survived * np.log2(p_not_survived)\n",
    "z=p_survived+p_not_survived\n",
    "print(z)\n",
    "t3 = -z * np.log2(z)\n",
    "print(t3)\n",
    "t = - p_survived * np.log2(p_survived)\n",
    "t2 = - p_not_survived * np.log2(p_not_survived)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad947a",
   "metadata": {},
   "source": [
    "**Task 1:**\n",
    "\n",
    "Recompute the root node entropy. On Pen & paper or with just some lines of code here using basic mathematical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8f14e",
   "metadata": {},
   "source": [
    "survived = $342$\n",
    "\n",
    "pSurvived = $\\frac{342}{891}$\n",
    "\n",
    "pNotSurvived = $1-$ pSurvived = $1-0.3838383838383838=0.6161616161616161$\n",
    "\n",
    "entropyBeforeSplit = $-pSurvived \\cdot \\log_2(pSurvived) - pNotSurvived \\cdot \\log_2(pNotSurvived)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89760968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6161616161616161"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "342/891\n",
    "1-0.3838383838383838\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330275e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf0258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
