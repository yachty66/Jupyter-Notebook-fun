{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-Klausur \"Wissensrepräsentation\" - 1. Prüfungszeitraum\n",
    "\n",
    "Datum: 13. Juli 2021    \n",
    "Zeit: 9:45 - 11:15\n",
    "\n",
    "Schreiben Sie zuerst Ihre Namen und ihre Matrikelnummer in das Notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: \n",
    "\n",
    "Matrikelnummer:\n",
    "\n",
    "Bei der von mir eingereichten Arbeit handelt es sich um eine eKlausur auf Distanz. Die eKlausur wird open book durchgeführt. Das bedeutet, dass öffentlich zugängliche Materialien (wie Internetquellen, Bücher, Zeitschriften) und persönliche Aufzeichnungen verwenden werden dürfen. \n",
    "\n",
    "Durch die Abgabe versichere ich, dass ich die von mir eingereichte Arbeit selbstständig ohne fremde Hilfe verfasst zu haben und dass ich prüfungsfähig bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hinweise\n",
    "\n",
    "- Die Zeit für die Bearbeitung der Klausur ist extra sehr knapp bemessen. Diese Vorgabe \n",
    "wird gemacht, damit möglichst wenig Zeit für Kommunikation zwischen Studierenden und Recherche im Internet bleibt.\n",
    "\n",
    "- Bei teilweise identischen Lösungen zwischen Studierenden muss von einem Betrugsversuch ausgegangen werden. Die Strafen hierfür gehen von einer Bewertung mit 5 (nicht-bestanden) bis hin zur Exmatrikulation.\n",
    "\n",
    "- Eine zu spät abgegebene Klausur muss mit einer 5 (nicht-bestanden) gewertet werden. Laden Sie die Klausur daher rechtzeitig in moodle hoch.\n",
    "\n",
    "- Formeln sind im Latexstil zu setzen (in doppelten \\$\\$), z.B.\n",
    "\n",
    "$$\n",
    "A = \\frac{x^2}{\\sqrt{y}}\n",
    "$$\n",
    "- Nicht-korrekt gesetzte Formeln können nicht gewertet werden.\n",
    "- Alternativ kann die Lösung auf eingescanntes DIN A4-Blatt geschrieben werden. Schreiben Sie sauber und deutlich. Nicht lesbare Lösungen (auch schlechter Scan, schlechtes Foto) können nicht gewertet werden.\n",
    "- Alle Rechenwege sind anzugeben. Ihre Lösungen müssen nachvollziehbar und klar sein.\n",
    " Nicht kreuz und quer schreiben! Nicht nachvollziehbare Lösungen führen zu Punktabzug bzw. Nicht-Wertung.\n",
    "- Wenn Zahlenwerte in der Aufgabenstellung angegeben sind, müssen Sie\n",
    "diese verwenden und ein konkretes Ergebnis (Zahlenwert) produzieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ändern Sie nicht diese Zelle \n",
    "import numpy as np\n",
    " \n",
    "\n",
    "num_aufgaben = 7\n",
    "aufgaben_punkte = np.ndarray(num_aufgaben)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ändern Sie nicht diese Zelle \n",
    "\n",
    "#(Max.) Punkte für Aufgabe 1\n",
    "aufgabe = 1\n",
    "aufgaben_punkte[aufgabe -1] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1. Aufgabe: Kostenfunktion und Gradient\n",
    "\n",
    "\n",
    "\n",
    "Gegeben ist das lineare Modell\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "\n",
    "und folgende Trainingsdaten:\n",
    "\n",
    "$$\n",
    "D_{train} = \\{(-2,4),(0,0),(1,0),(2,-3)\\}\n",
    "$$\n",
    "\n",
    "mit den Tuplen $(x , y $: \n",
    "$x$ ist das Merkmal und $y$ der Zielwert.\n",
    "\n",
    "\n",
    "Für die Startwerte $\\theta_0 = 1$ und $\\theta_1 = -2$ berechnen Sie die folgenden Unteraufgaben:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Kosten\n",
    "\n",
    "Berechnen Sie die Kosten für das obige Beispiel\n",
    "$$ J_D(\\theta_0, \\theta_1)=\\frac{1}{2m}\\sum_{i=1}^{m}{(h_\\theta(x_i)-y_i)^2} $$\n",
    "\n",
    "mit den Werten aller Trainingsdaten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "\n",
    "$ J_D(\\theta_0, \\theta_1)=\\frac{1}{2m}\\sum_{i=1}^{m}{(h_\\theta(x_i)-y_i)^2} = \\frac{1}{2\\cdot4}\\cdot((1)^2+(1)^2+(-1)^2+(-0)^2)=\\frac{3}{8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Gradient\n",
    "\n",
    "Berechnen Sie die beiden partiellen Ableitungen \n",
    "$$ \\frac{\\partial J (\\theta_0, \\theta_1)}{\\partial \\theta_0} $$\n",
    "und\n",
    "$$ \\frac{\\partial J (\\theta_0, \\theta_1)}{\\partial \\theta_1} $$\n",
    "\n",
    "\n",
    "mit den Werten aller Trainingsdaten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "$$ \\frac{\\partial J (\\theta_0, \\theta_1)}{\\partial \\theta_0}=\\frac{1}{m}\\sum_{i=1}^{m}(\\theta_0 + \\theta_1 x^i - y^i)=\\frac{1}{4}\\cdot((1)+(1)+(-1)+(0))=\\frac{1}{4}$$\n",
    "\n",
    "$$\\frac{\\partial J (\\theta_0, \\theta_1)}{\\partial \\theta_1}=\\frac{1}{m}\\sum_{i=1}^{m}((\\theta_0 + \\theta_1 x^i - y^i)\\cdot x_i)=\\frac{1}{4}\\cdot (-2+0+(-1)+(0))=-\\frac{3}{4}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Gradientenabstiegsschritt\n",
    "\n",
    "Berechnen Sie die Parameterwerte $\\theta_0$ und $\\theta_1$ nach dem ersten Gradientenabstiegsschritt mit der Lernrate $\\alpha = 0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "$$\\theta_{j}^{new} \\leftarrow \\theta_{j}^{new} - \\alpha * \\frac{\\partial}{\\partial\\theta_{j}} J(\\theta^{old})$$\n",
    "\n",
    "$$\\theta_{0}=1 - 0.01 \\cdot \\frac{1}{4}=0.9975$$\n",
    "\n",
    "$$\\theta_{1}=-2 - 0.01 \\cdot -\\frac{3}{4}=-1.9925$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ändern Sie nicht diese Zelle \n",
    "\n",
    "#(Max.) Punkte für Aufgabe 2\n",
    "aufgabe = 2\n",
    "aufgaben_punkte[aufgabe -1] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2\n",
    "\n",
    "Diskutieren Sie folgende Fragestellung. \n",
    "\n",
    "Sie erhalten auf den Trainingsdaten die Genauigkeit (Accuracy) von 0.9 und auf den Validierungsdaten eine \n",
    "Genauigkeit von 0.86 für ein festes Modell (z.B. Standardeinstellungen eines scikit-learn Klassifizierer, \n",
    "nach völlständigem Training).\n",
    "\n",
    "Für Ihre Anwendung wollen Sie eine Genauigkeit von 0.92 erzielen.\n",
    "\n",
    "Ist davon auszugehen, dass mit mehr Trainingsdaten die gewünschte Genauigkeit \n",
    "von 0.92 erreicht werden kann? Begründen Sie Ihre Entscheidung.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ja, weil durch neue Traingsdaten dem Model neue Informationen hinzugefügt werden, wenn sie kein rauschen besitzen.\n",
    "\n",
    "Durch dem hinzufügen neuer Daten und der dadurch erzeugten Diversität reduzieren sich die Wahrscheinlichkeiten auf overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ändern Sie nicht diese Zelle \n",
    "\n",
    "#(Max.) Punkte für \n",
    "aufgabe = 3\n",
    "aufgaben_punkte[aufgabe -1] = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Aufgabe: Implementierung des Gradientenabstiegsverfahrens\n",
    "\n",
    "Folgender Code wird benutzt bei der Implementation des Gradientenabstiegsverfahren \n",
    "bei der univariaten lineare Regression:\n",
    "    \n",
    "```for i in range(nb_steps):\n",
    "        theta[1] = theta[1] - alpha * compute_gradient_cost(theta, 1)\n",
    "        theta[0] = theta[0] - alpha * compute_gradient_cost(theta, 0)```\n",
    "\n",
    "\n",
    "Dabei besteht `theta` aus den beiden Komponenten `theta[0]` und `theta[1]`. \n",
    "Falls z.B. `theta[1]` geändert wird, ändert sich die entsprechende Komponente von `theta`.\n",
    "\n",
    "`compute_gradient_cost(theta, x)` \n",
    "berechnet die partielle Ableitung der\n",
    "Kostenfunktion nach der entsprechenden Komponente 0 oder 1.\n",
    "\n",
    "\n",
    "Welcher prinzipielle Fehler ist hier vorhanden? Korrigieren Sie den (pseudo) Code.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "`for i in range(nb_steps):\n",
    "        theta[0] = theta[0] - alpha * compute_gradient_cost(theta, 0)\n",
    "        theta[1] = theta[1] - alpha * compute_gradient_cost(theta, 1)\n",
    "       `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ändern Sie nicht diese Zelle \n",
    "\n",
    "#(Max.) Punkte für\n",
    "aufgabe = 4\n",
    "aufgaben_punkte[aufgabe -1] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Aufgabe\n",
    "\n",
    "Sind die Aussagen richtig? Kommentieren und korrigieren Sie gegebenenfalls die Aussagen. \n",
    "Keine Punkte ohne Begründung!\n",
    "\n",
    "\n",
    "Folgende Aussagen beziehen sich auf die lineare bzw. logistische Regression, wie sie in der Veranstaltung besprochen \n",
    "wurden:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. \n",
    "Je höher die Lernrate $\\alpha$ ist, desto langsamer findet das Gradientenabstiegsverfahren das Minimum der Kostenfunktion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "Nein, desto schneller findet das Gradientenabstiegsverfahren das Minimum der Kostenfunktion (wenn es gefunden wird bei zu großen Schritten)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 \n",
    "Falls die Lernrate zu groß gewählt wird, divergiert das Gradientenabstiegsverfahren, d.h. der Kostenwert geht mit der Anzahl der Iterationen gegen Unendlich."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "Ja, weil durch die großen Schritte kein Minimum gefunden werden kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 \n",
    "\n",
    "Die Lernrate muss während des Trainings erniedrigt werden, damit das Gradientenabstiegsverfahren konvergieren kann."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "Nein, es ist auch möglich mit einer gleichbleibenden Lernrate (Anfangskonfiguration) konvergieren des Gradientenabstiegsverfahren zu erzielen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4\n",
    "\n",
    "Falls das Gradientenabstiegsverfahren konvergiert, bestimmt die Lernrate $\\alpha$ die Lage des Minimums im Parameterraum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "Ja, die Lernrate ist dafür verantwortlich wie weit zwischen den Schritten des Gradientenabstiegsverfahren gegangen wird und somit ist sie auch verantwortlich für das Ergebnis mit dem bestimmten Parameterraum im Minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 \n",
    "\n",
    "Wird Feature-Scaling angewandt, erhält man andere Werte für die Parameter, als ohne Feature-Scaling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "Ja, weil mit feature scaling zum Beispiel vermieden werden kann in einem lokalen Optimum stehen zu bleiben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ändern Sie nicht diese Zelle \n",
    "\n",
    "#(Max.) Punkte für Aufgabe \n",
    "aufgabe = 5\n",
    "aufgaben_punkte[aufgabe -1] = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 5: Lineare Separabilität\n",
    "\n",
    "Gegeben ist folgendes Klassifikationsproblem:\n",
    "\n",
    "Es gibt nur zwei binäre Features $x_1$ und $x_2$ mit $x_i \\in \\{0,1\\}$.\n",
    "\n",
    "Für den Zielwert gilt: $y \\in \\{0, 1\\}$.\n",
    "\n",
    "Dabei gilt folgende Klassenzuordnung $\\vec{x} \\rightarrow y$:\n",
    "- $(0,0)\\rightarrow 0$\n",
    "- $(0,1)\\rightarrow 1$\n",
    "- $(1,0)\\rightarrow 1$\n",
    "- $(1,1)\\rightarrow 0$\n",
    "\n",
    "Ist dieses Problem ein linear separables Klassifikationsproblem. Begründen Sie Ihre Entscheidung.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ihre Lösung hier oder auf einem separatem Blatt\n",
    "\n",
    "Ja, lineare separabilität ist die Trennung von zwei Mengen mit einer linearen Funktion. Beim gegebenen Beispiel bilden sich genau zwei Mengen ab, die linear separiert werden können.\n",
    "\n",
    "Eine Menge die $0$ abbildet:\n",
    "\n",
    "- $(0,0)\\rightarrow 0$\n",
    "- $(1,1)\\rightarrow 0$\n",
    "\n",
    "Und eine Menge die $1$ abbildet:\n",
    "\n",
    "- $(0,1)\\rightarrow 1$\n",
    "- $(1,0)\\rightarrow 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ändern Sie nicht diese Zelle \n",
    "\n",
    "#(Max.) Punkte für Aufgabe \n",
    "aufgabe = 6\n",
    "aufgaben_punkte[aufgabe -1] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 6: Kostenfunktion für log. Regression\n",
    "\n",
    "Gegeben sind folgende Trainingsdaten für die logistische Regression:\n",
    "\n",
    "$$\n",
    "\\mathcal D_{train} = \n",
    "\\left\\{ \n",
    "  (\\vec x^{(1)}, y^{(1)}), \n",
    "  (\\vec x^{(2)}, y^{(2)}), \n",
    "  (\\vec x^{(3)}, y^{(3)}),\n",
    "  (\\vec x^{(4)}, y^{(4)}) \n",
    "\\right\\}\n",
    "$$\n",
    "\n",
    "mit \n",
    "- $(\\vec x^{(1)}, y^{(1)}) = \\left( (3.7, 2.1), 1 \\right)$\n",
    "- $(\\vec x^{(2)}, y^{(2)}) = \\left( (1.1, 1.2), 0 \\right)$\n",
    "- $(\\vec x^{(3)}, y^{(3)}) = \\left( (4.0, 2.0), 1 \\right)$\n",
    "- $(\\vec x^{(4)}, y^{(4)}) = \\left( (0.9, 1.4), 0 \\right)$\n",
    "\n",
    "\n",
    "mit der Hypothese für den \"aktuellen\" Parametervektor \n",
    "$\\vec \\theta= (-3., 1.1, 1.2)$. Dies ergibt folgende Hypothesenwerte\n",
    "\n",
    "- $h^{(1)}(\\vec x^{(1)}) = 0.973$\n",
    "- $h^{(2)}(\\vec x^{(2)}) = 0.413$\n",
    "- $h^{(3)}(\\vec x^{(3)}) = 0.978$\n",
    "- $h^{(4)}(\\vec x^{(4)}) = 0.418$ \n",
    "\n",
    "\n",
    "Wie berechnet sich die typische Kostenfunktion der logistischen Regression für die oben genannten Werte? Vereinfachen Sie den Ausdruck so weit wie möglich.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4052646132241667"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1/4*((1*np.log2(0.973)+(1-1)*np.log2(1-0.973))+(0*np.log2(0.413)+(1-0)*np.log2(1-0.413))+(1*np.log2(0.978)+(1-1)*np.log2(1-0.978))+(0*np.log2(0.418)+(1-0)*np.log2(1-0.418)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "$$- \\frac{1}{m}  \\sum_{i=1}^{m} \n",
    "    \\left[  y^{(i)} \\log h_\\theta({\\vec x}^{(i)})+\n",
    "      (1 - y^{(i)}) \\log \\left( 1- h_\\theta({\\vec x}^{(i)})\\right) \\right] = -\\frac{1}{4}\\cdot \\bigg( (1 \\cdot \\log(0.973)+ (1-1)\\cdot \\log(1-0.973)\\bigg) +\\bigg((0 \\cdot \\log(0.413)+ (1-0)\\cdot \\log(1-0.413)\\bigg)+\\bigg((1 \\cdot \\log(0.978)+ (1-1)\\cdot \\log(1-0.978)\\bigg)+\\bigg((0 \\cdot \\log(0.418)+ (1-0)\\cdot \\log(1-0.418)\\bigg)=0.4052646132241667$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ändern Sie nicht diese Zelle \n",
    "\n",
    "#(Max.) Punkte für Aufgabe \n",
    "aufgabe = 7\n",
    "aufgaben_punkte[aufgabe -1] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 7. Entropie\n",
    "\n",
    "Gegeben ist folgende Split-Tabelle für ein Feature mit dem Schwellenwert $v$:\n",
    "\n",
    "|   | feature $\\geq v$  | feature < $v$ |\n",
    "|---|---|---|\n",
    "| Klasse 0  | 44  | 7  |\n",
    "| Klasse 1  | 12  |  88 |\n",
    "\n",
    "\n",
    "Nomenklatur:\n",
    "- Die Variable für die Klasse ist $Y$.\n",
    "- Die Variable für den Spilt mit dem Feature ist $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 7.1\n",
    "Berechnen Sie die Entropie vor dem Split mit dem Feature $H(Y)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "eBefore = -(51/100) * np.log2(51/151) - (100/151) * np.log2(100/151)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "$$H(Y)=-\\frac{51}{151} \\cdot \\log_2(\\frac{51}{151})-\\frac{100}{151}\\cdot \\log_2(\\frac{100}{151})=1.1923902539419529$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 7.2\n",
    "Berechnen Sie die bedingte Entropie nach dem Split mit dem Feature, d.h. $H(Y \\mid X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "$$E_{smaller}=-(\\frac{7}{95} \\cdot \\log_2(\\frac{7}{95})+\\frac{88}{95}\\cdot \\log_2(\\frac{88}{95})=0.3795243778626806$$\n",
    "\n",
    "$$E_{bigger}=-(\\frac{44}{56} \\cdot \\log_2(\\frac{44}{56})+\\frac{12}{56}\\cdot \\log_2(\\frac{12}{56})=0.74959525725948$$\n",
    "\n",
    "$$H(Y|X)=\\frac{95}{151} \\cdot E_{smaller} + \\frac{56}{151}\\cdot E_{bigger}=0.5167692073078513$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3795243778626806\n",
      "0.74959525725948\n",
      "0.5167692073078513\n"
     ]
    }
   ],
   "source": [
    "samples = 44 + 7 + 12 + 88\n",
    "entropy_feature_smaller_v = -(7/95*np.log2(7/95)+88/95*np.log2(88/95))\n",
    "entropy_feature_bigger_v = -(44/56*np.log2(44/56)+12/56*np.log2(12/56))\n",
    "print(entropy_feature_smaller_v)\n",
    "print(entropy_feature_bigger_v)\n",
    "condP = 95./(samples) * entropy_feature_smaller_v + 56./(samples) * entropy_feature_bigger_v \n",
    "print(condP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aufgabe 7.3\n",
    "\n",
    "Berechen Sie den Information Gain für den Split mit dem Feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "$$I = H(Y)-H(Y|X) = 0.6756210466341016$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6756210466341016\n"
     ]
    }
   ],
   "source": [
    "iGain = eBefore-condP\n",
    "print(iGain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "\n",
    "#### Ende der Klausur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.  5.  4. 10.  3.  5. 12.]\n",
      "\n",
      "Punkte für Aufgaben: 53.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Maximale Punktzahl\n",
    "print(aufgaben_punkte)\n",
    "\n",
    "print(\"\\nPunkte für Aufgaben:\", aufgaben_punkte.sum())\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
