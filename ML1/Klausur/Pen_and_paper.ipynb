{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e935c38",
   "metadata": {},
   "source": [
    "# Cost and Gradient univariate regression/simple regression\n",
    "\n",
    "Given the linear model:\n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta_0 + \\theta_1 x\n",
    "$$\n",
    "\n",
    "And the following concrete training data:\n",
    "\n",
    "$$\n",
    "D_{train} = \\{(0,1),(1,3),(2,6),(4,8)\\}\n",
    "$$\n",
    "\n",
    "with each tuple $(x,y)$ denoting $x$ the feature and $y$ the target.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "For $\\theta_0 = 1$ and $\\theta_1 = 2$ calculate:\n",
    "\n",
    "1. The cost:\n",
    "\n",
    "$$ J_D(\\theta_0, \\theta_1)=\\frac{1}{2m}\\sum_{i=1}^{m}{(h_\\theta(x^{(i)})-y^{(i)})^2} $$\n",
    "\n",
    "2. The gradient $\\nabla J$, i.e. the partial derivatives:\n",
    "\n",
    "$$ \\frac{\\partial J (\\theta_0, \\theta_1)}{\\partial \\theta_0} $$\n",
    "\n",
    "$$ \\frac{\\partial J (\\theta_0, \\theta_1)}{\\partial \\theta_1} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb7fd99",
   "metadata": {},
   "source": [
    "1. Cost:\n",
    "$$J_D(\\theta_0, \\theta_1)=\\frac{1}{2m}\\sum_{i=1}^{m}{(h_\\theta(x^{(i)})-y^{(i)})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53f5231",
   "metadata": {},
   "source": [
    "2. Gradient:\n",
    "\n",
    "$$\\frac{\\partial J (\\theta_0, \\theta_1)}{\\partial \\theta_0} =  \\frac{1}{m}\\sum_{i=1}^{m}(\\theta_0 + \\theta_1 x^i - y^i)$$\n",
    "\n",
    "$$\\frac{\\partial J (\\theta_0, \\theta_1)}{\\partial \\theta_0} = \\frac{1}{m}\\sum_{i=1}^{m}((\\theta_0 + \\theta_1 x^i - y^i) \\cdot x^i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725940be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4128c5ec",
   "metadata": {},
   "source": [
    "# Logistic regression and regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e74da",
   "metadata": {},
   "source": [
    "### Pen & Paper Exercises\n",
    "\n",
    "#### Task\n",
    "\n",
    "Why is \n",
    "\n",
    "$$\n",
    "\\text{arg}\\max_x f(x) = \\text{arg}\\min_x \\left[ - \\log f(x) \\right] \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f61801",
   "metadata": {},
   "source": [
    "#### Solution\n",
    " \n",
    "The logarithm is a strictly monotonic function so we have \n",
    "$$\n",
    "\\text{arg}\\max_x f(x) = \\text{arg}\\max_x \\left[ \\log f(x) \\right] \n",
    "$$\n",
    "\n",
    "with \n",
    "$$\n",
    " \\text{arg}\\min_x \\left[ - \\log f(x) \\right] = \\text{arg}\\max_x \\left[ \\log f(x) \\right] \n",
    "$$\n",
    "\n",
    "we have the result\n",
    "\n",
    "$$\n",
    "\\text{arg}\\max_x f(x) = \\text{arg}\\min_x \\left[ - \\log f(x) \\right] \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2fbfaa",
   "metadata": {},
   "source": [
    "#### Logistic model\n",
    "\n",
    "In logistic regression, the prediction of a learned model $h_\\Theta(\\vec x)$\n",
    "can be interpreted as the prediction that $\\vec x$ belongs to the positive class $1$:\n",
    "\n",
    "$$p(y=1\\mid \\vec x; \\Theta) = h_\\Theta(\\vec x)$$\n",
    "\n",
    "#### Task\n",
    "What is the probability of the negative class $p(y=0\\mid \\vec x; \\Theta)$ prediction (expressed with $h_\\Theta(\\vec x)$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4544a4",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "$$p(y=0\\mid \\vec x; \\Theta) = 1 - h_\\Theta(\\vec x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f533b63",
   "metadata": {},
   "source": [
    "#### Loss\n",
    "\n",
    "\n",
    "The loss of an example $(\\vec x^{(i)}, y^{(i)})$ with target value $y^{(i)}=1$ is\n",
    "$$loss_{(\\vec x^{(i)}, 1)} (\\Theta) = - \\log p(y=1\\mid \\vec x; \\Theta)$$\n",
    "\n",
    "The loss of an example $(\\vec x^{(i)}, y^{(i)})$ with target value $y^{(i)}=0$ is\n",
    "$$loss_{(\\vec x^{(i)}, 0)} (\\Theta) = - \\log p(y=0\\mid \\vec x; \\Theta)$$\n",
    "\n",
    "So, $p(y=k\\mid \\vec x; \\Theta)$ is maximized for the target class $k$ \"by searching\n",
    "in the $\\Theta$-space\".  \n",
    "\n",
    "$p(y=k\\mid \\vec x; \\Theta)$ is called *likelihood* of $\\Theta$ (of one example $(\\vec x, y)$)\n",
    "if it is considered as a function of $\\Theta$. \n",
    "Note that the likelihood is a function of $\\Theta$.\n",
    "\n",
    "$\\mathcal L^{(i)}(\\Theta) = \\log p(y=y^{(i)}\\mid \\vec x^{(i)}; \\Theta)$ is the log-likelihood\n",
    "of $\\Theta$ for an example $i$.\n",
    "\n",
    "Why is $p(y=k\\mid \\vec x; \\Theta)$ not a probability with respect to $\\Theta$.\n",
    "Which property of a probability does not hold?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b214af6c",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "$p(y=k\\mid \\vec x; \\Theta)$ is a probability with respect to $y$. The property\n",
    "$\\sum_k p(y=k\\mid \\vec x; \\Theta)=1$ holds.\n",
    "\n",
    "This propery does not hold with respect to $\\Theta$. \n",
    "The integral ($\\Theta$ is continuous) $$\\int_\\Theta p(y\\mid \\vec x; \\Theta) d\\Theta$$\n",
    "is not $1$ in general. Therefore, it's not a probability with respect to $\\Theta$. $\\Theta$ is on the right site of the conditioning-bar \"$\\mid$\".\n",
    "\n",
    "Therefore, it has another name if $p(y=k\\mid \\vec x; \\Theta)$ is considered w.r.t. $\\Theta$.\n",
    "The technical term **likelihood** is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366fec0c",
   "metadata": {},
   "source": [
    "#### i.i.d. and log-likelihood for all data\n",
    "\n",
    "Note that the training data in logistic regression should be \n",
    "**i.i.d.** (independent and identically distributed):\n",
    "\n",
    "An simple example of an i.i.d. data set is the toin coss of a (marked) coin.\n",
    "Assume that the probability of head (class $y=1$) is $0.4$, i.e. $p(y=1)=0.4$.     \n",
    "The probability of getting two heads in two throws is $0.4 \\cdot 0.4$:\n",
    "- Each throw has the same distribution (here: $p(y=1)=0.4$. Each throw of the same coin is **identically distributed**\n",
    "- The throws are **independent**. If we get a head on the first throw the probability of\n",
    "getting a head on the second throw does not change.\n",
    "\n",
    "So, the probability factorizes: $p(y^{(1)}=1, y^{(2)}=1)=p(y^{(1)}=1)p(y^{(2)}=1)$\n",
    "\n",
    "For our classification problem:\n",
    "\n",
    "$p(\\mathcal D_y \\mid \\mathcal D_x; \\Theta) = \\prod_i p(y=y^{(i)}\\mid \\vec x^{(i)}; \\Theta)$ \n",
    "\n",
    "with \n",
    "- $\\mathcal D_x= \\{x^{(1)}, x^{(2)}, \\dots , x^{(m)}\\}$\n",
    "- $\\mathcal D_y= \\{y^{(1)}, y^{(2)}, \\dots , y^{(m)}\\}$\n",
    "- $\\mathcal D$ is the combination of $\\mathcal D_x$ with $\\mathcal D_y$:\n",
    "$\\mathcal D= \\{ (\\vec x^{(1)},y^{(1)}), (\\vec x^{(2)},y^{(2)}), \\dots , (\\vec x^{(m)},y^{(m)})\\}$. \n",
    "\n",
    "#### Task \n",
    "For the whole data set the log-likelihood $\\mathcal L_\\mathcal D(\\Theta)$ of a parameter set $\\Theta$ is \n",
    "$\\log p(\\mathcal D_y \\mid \\mathcal D_x; \\Theta)$).     \n",
    "Note: The (log-)likelihood $\\mathcal L_\\mathcal D(\\Theta)$ is a function of the parameters $\\Theta$.\n",
    "Never say the (log-)likelihood of the data.\n",
    "\n",
    "1. What is $\\mathcal L_\\mathcal D(\\Theta) = \\log p(\\mathcal D_y \\mid \\mathcal D_x; \\Theta)$ expressed by the $p(y=y^{(i)}\\mid \\vec x^{(i)}; \\Theta)$?\n",
    "\n",
    "2. What is the relation of the log-likelihood $\\mathcal L^{(i)}(\\Theta)$ (for the individual examples $(\\vec x^{(i)}, y^{(i)})$) \n",
    "to the log-likelihood $\\mathcal L_\\mathcal D(\\Theta)$ for the whole data set.\n",
    "\n",
    " \n",
    "In logistic regression the cost function is the negative log-likelihood divided by the number of data examples $m$:\n",
    "\n",
    "$$J (\\Theta) = - \\frac{\\mathcal L_\\mathcal D(\\Theta)}{m}$$\n",
    "\n",
    "2. The aveage log-liklihood \n",
    "\n",
    "2. What is the relation of the (log-)likelihood with the cost function for logistic-regression? \n",
    "3. Derive the cost function of logistic-regression by using your result of 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da5684a",
   "metadata": {},
   "source": [
    "#### Solution\n",
    "\n",
    "1. \n",
    "$$ \\mathcal L_\\mathcal D(\\Theta) = \\log p(\\mathcal D_y \\mid \\mathcal D_x; \\Theta) = \\log \\prod_i p(y=y^{(i)}\\mid \\vec x^{(i)}; \\Theta) =\\sum_i \\log p(y=y^{(i)}\\mid \\vec x^{(i)}; \\Theta)  $$\n",
    "\n",
    "2. from 1. we have\n",
    "\n",
    "$$ \\mathcal L_{\\mathcal D}(\\Theta) = \\sum_i \\mathcal L^{(i)}(\\Theta) $$\n",
    "\n",
    "3.\n",
    "So $J (\\Theta)$ is the negative average of $\\mathcal L^{(i)}(\\Theta)$.\n",
    "\n",
    "\n",
    "$$J (\\Theta) = - \\frac{\\mathcal L_\\mathcal D(\\Theta)}{m}= - \\frac{1}{m} \\sum_i \\mathcal L^{(i)}(\\Theta)$$\n",
    "\n",
    "\n",
    "So, we have with $p(y=1\\mid \\vec x^{(i)}; \\Theta) = h_\\Theta(\\vec x)$ \n",
    "and $p(y=1\\mid \\vec x^{(i)}; \\Theta) = 1- h_\\Theta(\\vec x)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J (\\Theta) \n",
    " & = - \\frac{1}{m}  \\sum_i \\log p(y=y^{(i)}\\mid \\vec x^{(i)}; \\Theta) \\\\\n",
    " &= - \\frac{1}{m}  \\sum_{i=1}^{m} \n",
    "    \\left[  y^{(i)} \\log h_\\theta({\\vec x}^{(i)})+\n",
    "      (1 - y^{(i)}) \\log \\left( 1- h_\\theta({\\vec x}^{(i)})\\right) \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The trick of multiplying each term by $y^{(i)}$ resp. $(1 - y^{(i)})$ selects the correct term and\n",
    "cancels out the incorrect one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ed47b3",
   "metadata": {},
   "source": [
    "#### Derivative of the logistic function\n",
    "\n",
    "The sigmoid activation function is defined as $\\sigma (z) = \\frac{1}{1+\\exp(-z)}$ \n",
    "\n",
    "**Task:**\n",
    "\n",
    "Show that:\n",
    "$$\n",
    "\\frac{d \\sigma(z)}{d z} = \\sigma(z)(1-\\sigma(z))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e439da0",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{d \\sigma(z)}{d z} & = \\frac{d }{d z}  \\left(\\frac{1}{1+\\exp(-z)}\\right) \\\\\n",
    " & \\\\\n",
    " & \\text{Quotient rule}\\\\\n",
    " & \\\\\n",
    " & = \\frac{(1)'(1+\\exp(-z)) - (1)(1+\\exp(-z))'}{(1+\\exp(-z))^2} \\\\\n",
    " & \\\\\n",
    " & = \\frac{0(1+\\exp(-z)) - (1)(-\\exp(-z))}{(1+\\exp(-z))^2} \\\\\n",
    " & \\\\ \n",
    " & = \\frac{\\exp(-z)}{(1+\\exp(-z))^2} \\\\ \n",
    " & \\\\\n",
    " & \\text{adding +1-1 to the nominator}\\\\ \n",
    " & \\\\ \n",
    " & = \\frac{ 1 + \\exp(-z) - 1}{(1+\\exp(-z))^2} \\\\ \n",
    " & \\\\\n",
    " & = \\frac{1 + \\exp(-z) }{(1+\\exp(-z))^2} - \\frac{1}{(1+\\exp(-z))^2} \\\\\n",
    " & \\\\\n",
    " & = \\frac{1}{1+\\exp(-z)} - \\left( \\frac{1}{1+\\exp(-z)} \\right)^2 \\\\\n",
    " & \\\\\n",
    " & = \\sigma(z) - \\sigma(z)^2 \\\\\n",
    " & \\\\\n",
    " & = \\sigma(z) (1-\\sigma(z))\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653dcf4e",
   "metadata": {},
   "source": [
    "#### Task:\n",
    "\n",
    "Now show that:\n",
    "$$\n",
    "\\frac{\\partial \\sigma(z)}{\\partial \\theta_j} = \\sigma(z)(1-\\sigma(z)) \\cdot x_j\n",
    "$$\n",
    "\n",
    "\n",
    "with \n",
    "- $z=\\vec x'^T \\vec \\theta$\n",
    "\n",
    "and\n",
    "- $\\vec \\theta = (\\theta_0, \\theta_1, \\dots, \\theta_n)^T $\n",
    "- $\\vec x' = (x_0, x_1, \\dots, x_n)^T $\n",
    "\n",
    "\n",
    "Hint: Use the *chain rule of calculus*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b02333",
   "metadata": {},
   "source": [
    "Solution:\n",
    "    \n",
    "    \n",
    "Note that $z=\\vec x'^T \\vec \\theta = \\sum_{k=0}^{n} \\theta_k x_k$\n",
    "    \n",
    "$$\n",
    "\\frac{\\partial \\sigma(\\vec x'^T \\vec \\theta)}{\\partial \\theta_j} = \n",
    "\\frac{\\partial \\sigma(z)}{\\partial z} \\frac{\\partial z}{\\partial \\theta_j}=\n",
    "\\frac{\\partial \\sigma(z)}{\\partial z} \\frac{\\partial(\\sum_{k=0}^{n} \\theta_k x_k)}{\\partial \\theta_j}= \\sigma(z) (1-\\sigma(z)) x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f7c52",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "Show from\n",
    "$$\n",
    "    \\frac{\\partial}{\\partial \\theta_j}  J(\\theta)  =  \n",
    "    \\frac{\\partial}{\\partial \\theta_j}  \\left( - \\frac{1}{m}  \\sum_{i=1}^{m} \n",
    "    \\left[  y^{(i)} \\log h_\\theta({\\vec x}^{(i)})+\n",
    "      (1 - y^{(i)}) \\log \\left( 1- h_\\theta({\\vec x}^{(i)})\\right) \\right] \\right)\n",
    "$$  \n",
    "that\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_j}  J(\\theta)  =   \\frac{1}{m}\n",
    "     \\sum_{i=1}^{m} \\left( h_\\theta({\\vec x}^{(i)})- y^{(i)}\\right) x_j^{(i)}\n",
    "$$\n",
    "\n",
    "with the hypothesis $h_\\theta(\\vec x^{(i)}) = \\sigma(\\vec x'^T \\vec \\theta)$\n",
    "So, with our classification cost function (from the max-likelihood principle) the \n",
    "partial derivatives (components the gradient) has a simple form.\n",
    "\n",
    "**Hint:**\n",
    "\n",
    "1. Make use of your knowledge, that:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_\\theta(\\vec x^{(i)})}{\\partial \\theta_j} = h_\\theta(\\vec x^{(i)})(1-h_\\theta(\\vec x^{(i)})) \\cdot x_j\n",
    "$$\n",
    "2. and note that the chain rule for the derivative of the log is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\log(f(a))}{\\partial a} = \\frac{\\partial \\log(f(a))}{\\partial f} \\frac{\\partial f(a)}{\\partial a} =\n",
    "\\frac{1}{f(a)} \\frac{\\partial f(a)}{\\partial a}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda6a0f2",
   "metadata": {},
   "source": [
    "Solution (very, very explicitly - it's much simples as it looks like):\n",
    "\n",
    "$$\\begin{align}\n",
    "    \\frac{\\partial}{\\partial \\theta_j}  J(\\theta) &= \n",
    "    \\frac{\\partial}{\\partial \\theta_j}  \\left( - \\frac{1}{m}  \\sum_{i=1}^{m} \n",
    "    \\left[  y^{(i)} \\log h_\\theta({\\vec x}^{(i)})+\n",
    "      (1 - y^{(i)}) \\log \\left( 1- h_\\theta({\\vec x}^{(i)})\\right) \\right] \\right) \\\\ &=\n",
    "        - \\frac{1}{m}  \\sum_{i=1}^{m} \n",
    "    \\left[  y^{(i)} \\frac{\\partial}{\\partial \\theta_j}  \\log h_\\theta({\\vec x}^{(i)})+\n",
    "      (1 - y^{(i)}) \\frac{\\partial}{\\partial \\theta_j}  \\log \\left( 1- h_\\theta({\\vec x}^{(i)})\\right) \\right]  \\\\&=  \n",
    " - \\frac{1}{m}  \\sum_{i=1}^{m} \n",
    "    \\left[  y^{(i)} \\frac{h_\\theta({\\vec x}^{(i)}) (1-h_\\theta({\\vec x}^{(i)})) } {h_\\theta({\\vec x}^{(i)})} x_j^{(i)} -\n",
    "      (1 - y^{(i)}) \\frac{h_\\theta({\\vec x}^{(i)}) (1-h_\\theta({\\vec x}^{(i)})) }{\\left( 1- h_\\theta({\\vec x}^{(i)})\\right)} x_j^{(i)} \\right]\\\\\n",
    "    &=    - \\frac{1}{m}  \\sum_{i=1}^{m} \n",
    "    \\left[  y^{(i)} {  (1-h_\\theta({\\vec x}^{(i)})) }   x_j^{(i)} -\n",
    "      (1 - y^{(i)}) {h_\\theta({\\vec x}^{(i)}) } x_j^{(i)} \\right]\\\\\n",
    "    &=    - \\frac{1}{m}  \\sum_{i=1}^{m} \n",
    "    \\left[  y^{(i)} {  (1-h_\\theta({\\vec x}^{(i)})) }  -\n",
    "      (1 - y^{(i)}) {h_\\theta({\\vec x}^{(i)} }) \\right]x_j^{(i)}\\\\ \n",
    "    &=    - \\frac{1}{m}  \\sum_{i=1}^{m} \n",
    "    \\left[  y^{(i)} - y^{(i)} h_\\theta({\\vec x}^{(i)})   -\n",
    "      ( h_\\theta({\\vec x}^{(i)}) - y^{(i)} h_\\theta({\\vec x}^{(i)}) ) \\right]x_j^{(i)}\\\\\n",
    "    &=    - \\frac{1}{m}  \\sum_{i=1}^{m} \n",
    "    \\left[  y^{(i)} - y^{(i)} h_\\theta({\\vec x}^{(i)})   \n",
    "      - h_\\theta({\\vec x}^{(i)}) + y^{(i)} h_\\theta({\\vec x}^{(i)}) \\right]x_j^{(i)}\\\\\n",
    "    &=    - \\frac{1}{m}  \\sum_{i=1}^{m} \n",
    "    \\left[  y^{(i)}    \n",
    "      - h_\\theta({\\vec x}^{(i)})  \\right]x_j^{(i)}\\\\\n",
    "  &=  \\frac{1}{m}  \\sum_{i=1}^{m} \n",
    "    \\left[  h_\\theta({\\vec x}^{(i)}) - y^{(i)} \\right]x_j^{(i)}\n",
    "\\end{align}\n",
    "$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a565c419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
